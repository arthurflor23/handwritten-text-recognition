{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"name":"tutorial.ipynb","provenance":[],"collapsed_sections":["oMty1YwuWHpN"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/arthurflor23/handwritten-text-recognition/blob/master/src/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"gP-v0E_S-mQP"},"source":["<img src=\"https://github.com/arthurflor23/handwritten-text-recognition/blob/master/doc/image/header.png?raw=true\" />\n","\n","# Handwritten Text Recognition using TensorFlow 2.x\n","\n","This tutorial shows how you can use the project [Handwritten Text Recognition](https://github.com/arthurflor23/handwritten-text-recognition) in your Google Colab.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oMty1YwuWHpN"},"source":["## 1 Localhost Environment\n","\n","We'll make sure you have the project in your Google Drive with the datasets in HDF5. If you already have structured files in the cloud, skip this step."]},{"cell_type":"markdown","metadata":{"id":"39blvPTPQJpt"},"source":["### 1.1 Datasets\n","\n","The datasets that you can use:\n","\n","a. [Bentham](http://transcriptorium.eu/datasets/bentham-collection/)\n","\n","b. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n","\n","c. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n","\n","d. [Saint Gall](http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/saint-gall-database)\n","\n","e. [Washington](http://www.fki.inf.unibe.ch/databases/iam-historical-document-database/washington-database)"]},{"cell_type":"markdown","metadata":{"id":"QVBGMLifWQwl"},"source":["### 1.2 Raw folder\n","\n","On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n","\n","```\n",".\n","├── raw\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── preproc.py\n","    │   ├── reader.py\n","    │   ├── similar_error_analysis.py\n","    ├── main.py\n","    ├── network\n","    │   ├── architecture.py\n","    │   ├── layers.py\n","    │   ├── model.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","After that, create virtual environment and install the dependencies with python 3 and pip:\n","\n","> ```python -m venv .venv && source .venv/bin/activate```\n","\n","> ```pip install -r requirements.txt```"]},{"cell_type":"markdown","metadata":{"id":"WyLRbAwsWSYA"},"source":["### 1.3 HDF5 files\n","\n","Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n","\n","> ```python main.py --source=<DATASET_NAME> --transform```\n","\n","Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n","\n","\n","```\n",".\n","├── data\n","│   ├── bentham.hdf5\n","│   ├── iam.hdf5\n","│   ├── rimes.hdf5\n","│   ├── saintgall.hdf5\n","│   └── washington.hdf5\n","├── raw\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── preproc.py\n","    │   ├── reader.py\n","    │   ├── similar_error_analysis.py\n","    ├── main.py\n","    ├── network\n","    │   ├── architecture.py\n","    │   ├── layers.py\n","    │   ├── model.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","Then upload the **data** and **src** folders in the same directory in your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"jydsAcWgWVth"},"source":["## 2 Google Drive Environment\n"]},{"cell_type":"markdown","metadata":{"id":"wk3e7YJiXzSl"},"source":["### 2.1 TensorFlow 2.x"]},{"cell_type":"markdown","metadata":{"id":"Z7twXyNGXtbJ"},"source":["Make sure the jupyter notebook is using GPU mode."]},{"cell_type":"code","metadata":{"id":"mHw4tODULT1Z"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FMg-B5PH9h3r"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name != \"/device:GPU:0\":\n","    raise SystemError(\"GPU device not found\")\n","\n","print(\"Found GPU at: {}\".format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyMv5wyDXxqc"},"source":["### 2.2 Google Drive"]},{"cell_type":"markdown","metadata":{"id":"P5gj6qwoX9W3"},"source":["Mount your Google Drive partition.\n","\n","**Note:** *\\\"Colab Notebooks/handwritten-text-recognition/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."]},{"cell_type":"code","metadata":{"id":"ACQn1iBF9k9O"},"source":["from google.colab import drive\n","\n","drive.mount(\"./gdrive\", force_remount=True)\n","\n","%cd \"./gdrive/My Drive/Colab Notebooks/handwritten-text-recognition/src/\"\n","!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwogUA8RZAyp"},"source":["After mount, you can see the list os files in the project folder."]},{"cell_type":"markdown","metadata":{"id":"-fj7fSngY1IX"},"source":["## 3 Set Python Classes"]},{"cell_type":"markdown","metadata":{"id":"p6Q4cOlWhNl3"},"source":["### 3.1 Environment"]},{"cell_type":"markdown","metadata":{"id":"wvqL2Eq5ZUc7"},"source":["First, let's define our environment variables.\n","\n","Set the main configuration parameters, like input size, batch size, number of epochs and list of characters. This make compatible with **main.py** and jupyter notebook:\n","\n","* **dataset**: \"bentham\", \"iam\", \"rimes\", \"saintgall\", \"washington\"\n","\n","* **arch**: network to run: \"bluche\", \"puigcerver\", \"flor\"\n","\n","* **epochs**: number of epochs\n","\n","* **batch_size**: number size of the batch"]},{"cell_type":"code","metadata":{"id":"_Qpr3drnGMWS"},"source":["import os\n","import datetime\n","import string\n","\n","# define parameters\n","source = \"bentham\"\n","arch = \"flor\"\n","epochs = 1000\n","batch_size = 16\n","\n","# define paths\n","source_path = os.path.join(\"..\", \"data\", f\"{source}.hdf5\")\n","output_path = os.path.join(\"..\", \"output\", source, arch)\n","target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n","os.makedirs(output_path, exist_ok=True)\n","\n","# define input size, number max of chars per line and list of valid chars\n","input_size = (1024, 128, 1)\n","max_text_length = 128\n","charset_base = string.printable[:95]\n","\n","print(\"source:\", source_path)\n","print(\"output\", output_path)\n","print(\"target\", target_path)\n","print(\"charset:\", charset_base)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFextshOhTKr"},"source":["### 3.2 DataGenerator Class"]},{"cell_type":"markdown","metadata":{"id":"KfZ1mfvsanu1"},"source":["The second class is **DataGenerator()**, responsible for:\n","\n","* Load the dataset partitions (train, valid, test);\n","\n","* Manager batchs for train/validation/test process."]},{"cell_type":"code","metadata":{"id":"8k9vpNzMIAi2"},"source":["from data.generator import DataGenerator\n","\n","dtgen = DataGenerator(source=source_path,\n","                      batch_size=batch_size,\n","                      charset=charset_base,\n","                      max_text_length=max_text_length)\n","\n","print(f\"Train images: {dtgen.size['train']}\")\n","print(f\"Validation images: {dtgen.size['valid']}\")\n","print(f\"Test images: {dtgen.size['test']}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-OdgNLK0hYAA"},"source":["### 3.3 HTRModel Class"]},{"cell_type":"markdown","metadata":{"id":"jHktk8AFcnKy"},"source":["The third class is **HTRModel()**, was developed to be easy to use and to abstract the complicated flow of a HTR system. It's responsible for:\n","\n","* Create model with Handwritten Text Recognition flow, in which calculate the loss function by CTC and decode output to calculate the HTR metrics (CER, WER and SER);\n","\n","* Save and load model;\n","\n","* Load weights in the models (train/infer);\n","\n","* Make Train/Predict process using *generator*.\n","\n","To make a dynamic HTRModel, its parameters are the *architecture*, *input_size* and *vocab_size*."]},{"cell_type":"code","metadata":{"id":"nV0GreStISTR"},"source":["from network.model import HTRModel\n","\n","# create and compile HTRModel\n","model = HTRModel(architecture=arch,\n","                 input_size=input_size,\n","                 vocab_size=dtgen.tokenizer.vocab_size,\n","                 beam_width=10,\n","                 stop_tolerance=20,\n","                 reduce_tolerance=15)\n","\n","model.compile(learning_rate=0.001)\n","model.summary(output_path, \"summary.txt\")\n","\n","# get default callbacks and load checkpoint weights file (HDF5) if exists\n","model.load_checkpoint(target=target_path)\n","\n","callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1fnz0Eugqru"},"source":["## 4 Training"]},{"cell_type":"markdown","metadata":{"id":"w1mLOcqYgsO-"},"source":["The training process is similar to the *fit()* of the Keras. After training, the information (epochs and minimum loss) is save."]},{"cell_type":"code","metadata":{"id":"2P6MSoxCISlD"},"source":["# to calculate total and average time per epoch\n","start_time = datetime.datetime.now()\n","\n","h = model.fit(x=dtgen.next_train_batch(),\n","              epochs=epochs,\n","              steps_per_epoch=dtgen.steps['train'],\n","              validation_data=dtgen.next_valid_batch(),\n","              validation_steps=dtgen.steps['valid'],\n","              callbacks=callbacks,\n","              shuffle=True,\n","              verbose=1)\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","loss = h.history['loss']\n","val_loss = h.history['val_loss']\n","\n","min_val_loss = min(val_loss)\n","min_val_loss_i = val_loss.index(min_val_loss)\n","\n","time_epoch = (total_time / len(loss))\n","total_item = (dtgen.size['train'] + dtgen.size['valid'])\n","\n","t_corpus = \"\\n\".join([\n","    f\"Total train images:      {dtgen.size['train']}\",\n","    f\"Total validation images: {dtgen.size['valid']}\",\n","    f\"Batch:                   {dtgen.batch_size}\\n\",\n","    f\"Total time:              {total_time}\",\n","    f\"Time per epoch:          {time_epoch}\",\n","    f\"Time per item:           {time_epoch / total_item}\\n\",\n","    f\"Total epochs:            {len(loss)}\",\n","    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n","    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n","    f\"Validation loss:         {min_val_loss:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n","    lg.write(t_corpus)\n","    print(t_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"13g7tDjWgtXV"},"source":["## 5 Predict"]},{"cell_type":"markdown","metadata":{"id":"ddO26OT-g_QK"},"source":["The predict process is similar to the *predict* of the Keras:"]},{"cell_type":"code","metadata":{"id":"a9iHL6tmaL_j"},"source":["from data import preproc as pp\n","from google.colab.patches import cv2_imshow\n","\n","start_time = datetime.datetime.now()\n","\n","# predict() function will return the predicts with the probabilities\n","predicts, _ = model.predict(x=dtgen.next_test_batch(),\n","                            steps=dtgen.steps['test'],\n","                            ctc_decode=True,\n","                            verbose=1)\n","\n","# decode to string\n","predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n","ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","# mount predict corpus file\n","with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n","    for pd, gt in zip(predicts, ground_truth):\n","        lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\n","   \n","for i, item in enumerate(dtgen.dataset['test']['dt'][:10]):\n","    print(\"=\" * 1024, \"\\n\")\n","    cv2_imshow(pp.adjust_to_see(item))\n","    print(ground_truth[i])\n","    print(predicts[i], \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9JcAs3Q3WNJ-"},"source":["## 6 Evaluate"]},{"cell_type":"markdown","metadata":{"id":"8LuZBRepWbom"},"source":["Evaluation process is more manual process. Here we have the `ocr_metrics`, but feel free to implement other metrics instead. In the function, we have three parameters: \n","\n","* predicts\n","* ground_truth\n","* norm_accentuation (calculation with/without accentuation)\n","* norm_punctuation (calculation with/without punctuation marks)"]},{"cell_type":"code","metadata":{"id":"0gCwEYdKWOPK"},"source":["from data import evaluation\n","\n","evaluate = evaluation.ocr_metrics(predicts, ground_truth)\n","\n","e_corpus = \"\\n\".join([\n","    f\"Total test images:    {dtgen.size['test']}\",\n","    f\"Total time:           {total_time}\",\n","    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n","    f\"Metrics:\",\n","    f\"Character Error Rate: {evaluate[0]:.8f}\",\n","    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n","    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n","    lg.write(e_corpus)\n","    print(e_corpus)"],"execution_count":null,"outputs":[]}]}